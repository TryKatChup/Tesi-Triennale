\chapter{Implementazione}
\label{ch:implementazione}
\section{Allenamento di un modello di word embedding tramite FastText}
\subsection{Introduzione}
Lo scopo del progetto è sviluppare un classificatore che, date due password, determini se sono simili tra loro (e quindi potenzialmente non sicure) o meno.

A tal proposito sono stati implementati modelli di word embedding, i quali permettono di rappresentare parole in uno spazio vettoriale preservando proprietà semantiche quali la similarità e la co-occorrenza nel medesimo contesto.
Come modello è stato scelto FastText, il quale sfrutta informazioni sugli n-gram della parola per determinarne l'embedding~\cite{biijeta}.

Si è deciso di allenare 5 diversi modelli:
\begin{itemize}
    \item Il modello di Bijeeta et alii~\cite{biijeta} che utilizza la libreria \texttt{word2keypress} per ricavare la sequenza di tasti premuti per specifici caratteri, numero minimo di n-gram pari a 1, e numero di epoche per allenare la rete pari a 5.
    \item Un modello che utilizza \texttt{word2keypress}, con numero minimo di n-gram pari a 2, e numero di epoche per allenare la rete pari a 10.
    \item Tre modelli che non rilevano la sequenza di tasti premuti:
    \begin{itemize}
        \item Uno con numero minimo di n-gram pari a 1, e numero di epoche per allenare la rete pari a 5.
        \item Uno con numero minimo di n-gram pari a 2, e numero di epoche per allenare la rete pari a 5.
        \item Uno con numero minimo di n-gram pari a 2, e numero di epoche per allenare la rete pari a 10.
    \end{itemize}
\end{itemize}

\subsection{Precondizioni}
Prima della fase di allenamento della rete, è necessario avere un dataset valido di password; nel progetto è stato utilizzato lo stesso data breach da 45 GB citato da Bijeeta et alii~\cite{biijeta}, tuttavia sono stati scelti criteri diversi di filtraggio:
\begin{itemize}
    \item Sono state rimossi gli account con password con lunghezza inferiore a 4 caratteri o maggiore di 30.
    \item Sono state rimosse gli account con password che presentavano caratteri non ASCII o non stampabili.
    \item Sono stati rimossi gli account creati da bot, riconoscibili grazie al numero di occorrenze della stessa email nel dataset, superiore a 100.
    \item Sono stati rimossi gli account con password contententi sequenze esadecimali (identificate da \texttt{\$HEX[]} e \texttt{\textbackslash x}).
    \item Sono stati rimossi sequenze che rappresentano caratteri in HTML, come:
    \begin{itemize}
        \item \texttt{\&gt} (simbolo $>$);
        \item \texttt{\&ge} (simbolo $\geq$);
        \item \texttt{\&lt} (simbolo $<$)
        \item \texttt{\&le} (simbolo $\leq$);
        \item \texttt{\&\#} (ovvero i codici di entità in HTML);
        \item \texttt{amp}.
    \end{itemize}
    \item Sono stati rimossi gli account che presentavano meno di 2 password, poiché più password per utente risultano indispensabili per ricavare la similarità.
\end{itemize}
Successivamente, in accordo con Bijeeta et alii~\cite{biijeta}, per i modelli che prevedevano la memorizzazione delle sequenze di tasti premuti, è stata utilizzata la libreria python \href{https://github.com/rchatterjee/word2keypress}{\texttt{word2keypress}}.
Successivamente i risultati sono stati salvati in un file \texttt{csv} nel seguente formato:
\begin{center}
    \texttt{sample@gmail.com: ["'97314348'", "'voyager<s>1'"]}
\end{center}

\subsection{Impostazioni dell'ambiente}
Per potere allenare il modello è stato utilizzato \texttt{gensim.FastText}~\cite{gensim}.
\\
\texttt{gensim} è una libreria python multipiattaforma e open source che racchiude un'ampia scelta di modelli di word embedding pre allenati~\cite{gensim}.
Per estrapolare i dati dal file \texttt{csv} è stata sviluppata una classe ausiliaria \texttt{PasswordRetriever}.
\subsection{Allenamento del modello}
\subsubsection{Parametri di FastText}
\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=LightGray,
    fontsize=\footnotesize,
    linenos
    ]{python}
negative = 5
subsampling = 1e-3
min_count = 10
min_n = 2
max_n = 4
SIZE = 200
sg = 1
\end{minted}
Per questo progetto è stato utilizzato il modello \emph{Skip-gram} (\texttt{sg = 1}) e il negative sampling~\cite{negative}:
\begin{itemize}
    \item Il modello Skip-gram (parametro \texttt{sg = 1}) è stato scelto in quanto la rappresentazione distribuita dell'input è stata utilizzata per prevedere il contesto delle password. In particolare, risulta efficace per determinare quali caratteri intorno a una specifica sequenza sono presenti; in questo modo il modello riesce a imparare password non presenti nel corpus fornito per l'allenamento.~\cite{fasttext}
    \item Il negative sampling (parametro \texttt{negative = 5}) rende l'allenamento più veloce, poiché ciascuna sezione dell'allenamento aggiorna solo una piccola percentuale dei pesi del modello.~\cite{negative} Per dataset di dimensioni maggiori (come in questo caso) è consigliabile impostare il suo valore tra 2 e 5.~\cite{gensim}
    \item La dimensione del vettore contenente gli embedding è impostato a $200$ (parametro \texttt{SIZE = 200}), in modo da potere allenare più velocemente il modello. Normalmente viene raccomandata una dimensione pari a \texttt{SIZE = 300}.~\cite{gensim}
    \item Il subsampling ignora le password più frequenti, ovvero che presentano più di 1000 occorrenze.
    \item \texttt{mincount} rappresenta il numero minimo di occorrenze di una password nel dataset di training affinché venga considerata nell'allenamento~\cite{biijeta}~\cite{gensim}.
    \item \texttt{min\_n} e \texttt{max\_n} rappresentano rispettivamente il numero minimo e massimo degli n-gram. Gli N-gram vengono utilizzati per prevedere una sequenza di caratteri e il contesto di quest'ultima. In questo caso essi rappresentano una sequenza di caratteri contigui e il loro scopo è di dare informazioni di contesto e posizionali di una determinata sequenza all'interno di una password~\cite{biijeta}~\cite{gensim}.
\end{itemize}

\subsubsection{Allenare FastText}
La lista di password relativa a ciascun utente (\texttt{password\_list}) viene ottenuta grazie alla classe ausiliaria \texttt{PasswordRetriever}. Il modello di FastText si basa su \texttt{password\_list} e viene allenato con i parametri elencati precedentemente.

\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=LightGray,
    fontsize=\footnotesize,
    linenos
    ]{python}
filename='../train.csv'
password_list = PasswordRetriever(filename)
trained_model = FastText(password_list, size=SIZE, min_count=min_count,
                        workers=12, negative=negative,
                        sample=subsampling, window=20,
                        min_n=min_n, max_n=max_n)
\end{minted}

\subsection{Compressione del modello}
Il modello allenato ha un peso complessivo pari a 4.8 GB, e ciò comporta i seguenti problemi:
\begin{itemize}
    \item Meno efficiente in termini di spazio, di conseguenza l'utilizzo del modello è limitato su sistemi con vincoli di memoria o di spazio.
    \item È difficile utilizzare il modello in contesti distribuiti, poiché non è facilmente trasportabile.
\end{itemize}
Per comprimere il modello si è utilizzata la libreria \texttt{compress\_fasttext}, che sfrutta tecniche di quantizzazione e di feature selection.~\cite{compress-fasttext}
\begin{minted}[
    frame=lines,
    framesep=2mm,
    baselinestretch=1.2,
    bgcolor=LightGray,
    fontsize=\footnotesize,
    linenos
    ]{python}
big_model = gensim.models.fasttext.load_facebook_vectors('model.bin')
small_model = compress_fasttext.prune_ft_freq(big_model, pq=True)
small_model.save('compressed_model')
\end{minted}

Viene definito come \emph{feature selection} il processo di selezione delle feature più importanti da usare per costruire un modello~\cite{feature}.

Per \emph{Product quantization} si intende un particolare tipo di quantizzazione vettoriale, che viene utilizzata per la compressione di modelli di linguaggio naturale e di elaborazione di immagini e consente di generare in modo non esponenziale una quantità grande di codice in tempi contenuti e con costi ridotti in termini di memoria~\cite{biijeta}~\cite{compress-fasttext}~\cite{quantization}.

Il modello compresso ottenuto dalle operazioni di quantizzazione e di feature selection ha una dimensione di 20 MB.